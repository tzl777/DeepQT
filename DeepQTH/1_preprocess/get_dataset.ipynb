{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4da5f52-46a1-4bc2-95f0-e19a409192b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(\n",
      "  x=[72],\n",
      "  edge_index=[2, 2664],\n",
      "  edge_attr=[2664, 10],\n",
      "  stru_id='1',\n",
      "  voronoi_values=[72, 1],\n",
      "  centralities=[72, 1],\n",
      "  cart_coords=[72, 3],\n",
      "  lattice=[3, 3],\n",
      "  term_mask=[2664],\n",
      "  node_paths=[1, 72, 72, 5],\n",
      "  edge_paths=[1, 72, 72, 4],\n",
      "  term_real=[2664, 9, 9],\n",
      "  atom_num_orbital=[72],\n",
      "  subgraph_dict={\n",
      "    subgraph_atom_idx=[197136, 2],\n",
      "    subgraph_edge_idx=[197136],\n",
      "    subgraph_edge_ang=[197136, 16],\n",
      "    subgraph_index=[197136],\n",
      "  },\n",
      "  spinful=False\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch #用于将多个图数据合并成一个批处理，以便在图神经网络中进行有效的并行处理。\n",
    "import numpy as np\n",
    "import h5py\n",
    "import networkx as nx\n",
    "from spherical_harmonics_basis import get_spherical_from_cartesian, SphericalHarmonics, _spherical_harmonics\n",
    "\n",
    "from pymatgen.core.structure import Structure\n",
    "\n",
    "\"\"\"\n",
    "The function get_graph below is extended from \"https://github.com/materialsproject/pymatgen\", which has the MIT License below\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2011-2012 MIT & The Regents of the University of California, through Lawrence Berkeley National Laboratory\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "def get_graph(cart_coords, frac_coords, numbers, stru_id, radius, material_dimension, numerical_tol, lattice,\n",
    "              default_dtype_torch, tb_folder, data_format, num_l, shortest_path_length, if_lcmp_graph,\n",
    "              separate_onsite, target='hamiltonian', huge_structure=False, only_get_R_list=False, if_new_sp=False,\n",
    "              if_require_grad=False, fid_rc=None, **kwargs):\n",
    "    #cart_coords, frac_coords, [72*6], 1-600, 7.0, 2, 1e-8, lattice, default_dtype_torch, tb_folder当前文件路径, h5, 4, 5, True, False, hamiltonian, False, False, False, False, None\n",
    "    assert target in ['hamiltonian', 'phiVdphi', 'density_matrix', 'O_ij', 'E_ij', 'E_i']\n",
    "    if target == 'density_matrix' or target == 'O_ij':\n",
    "        assert data_format == 'h5' or data_format == 'h5_rc_only'\n",
    "    if target == 'E_ij':\n",
    "        assert data_format == 'h5'\n",
    "        assert separate_onsite is True\n",
    "    if target == 'E_i':\n",
    "        assert data_format == 'h5'\n",
    "        assert if_lcmp_graph is False\n",
    "        assert separate_onsite is True\n",
    "\n",
    "    assert tb_folder is not None\n",
    "    if data_format == 'h5_rc_only' and target == 'E_ij':\n",
    "        raise NotImplementedError\n",
    "    elif data_format == 'h5' or (data_format == 'h5_rc_only' and target != 'E_ij'): #True\n",
    "        key_atom_list = [[] for _ in range(len(numbers))] #创建第stru_id个结构的原子序数的空列表，长度为len(numbers)=72\n",
    "        edge_idx_target, edge_fea, edge_idx_source = [], [], []\n",
    "        if if_lcmp_graph: #True\n",
    "            atom_idx_connect, edge_idx_connect = [], []\n",
    "            edge_idx_connect_cursor = 0\n",
    "        if target == 'E_ij':\n",
    "            fid = h5py.File(os.path.join(tb_folder, 'E_delta_ee_ij.h5'), 'r')\n",
    "        else:\n",
    "            if if_require_grad: #False\n",
    "                fid = fid_rc\n",
    "            else:\n",
    "                fid = h5py.File(os.path.join(tb_folder, 'rc.h5'), 'r') #读取第stru_id个结构的所有原子的截断半径内的局域旋转坐标单位向量。\n",
    "        for k in fid.keys():\n",
    "            key = json.loads(k) #返回的是python的字典对象。\n",
    "            key_tensor = torch.tensor([key[0], key[1], key[2], key[3], key[4]]) # (R, i, j) i and j is 0-based index\n",
    "            if separate_onsite: #False\n",
    "                if key[0] == 0 and key[1] == 0 and key[2] == 0 and key[3] == key[4]:\n",
    "                    continue\n",
    "            key_atom_list[key[3]].append(key_tensor) #将第stru_id结构的每个原子i的截断半径内的所有原子key_tensor存入key_atom_list中，键代表72个原子，值代表每个原子i的所有邻居原子key_tensor列表\n",
    "        if target != 'E_ij' and not if_require_grad:\n",
    "            fid.close()\n",
    "        # print(\"cart_coords.shape, key_atom_list.shape = \", len(cart_coords), len(key_atom_list), len(key_atom_list[40])) #有36个原子坐标，第36号原子有44个邻居原子\n",
    "    \n",
    "        for index_first, (cart_coord, keys_tensor) in enumerate(zip(cart_coords, key_atom_list)): #第stru_id结构中的每个原子的笛卡尔坐标和该原子的邻居原子key_tensor列表，打包成一个可迭代对象，可迭代72次（原子个数次）。\n",
    "            keys_tensor = torch.stack(keys_tensor) #把张量列表凑成一个2维的张量；也就是在增加新的维度进行堆叠。stru_id结构的每个原子的邻居原子堆叠。44*5\n",
    "            cart_coords_j = cart_coords[keys_tensor[:, 4]] + keys_tensor[:, :3].type(default_dtype_torch).to(cart_coords.device) @ lattice.to(cart_coords.device)#指定希望在cart_coords.device设备上执行张量和模型操作。找出超胞内相邻原子j的实际笛卡尔坐标。53*3\n",
    "            dist = torch.norm(cart_coords_j - cart_coord[None, :], dim=1)#指定在哪个维度上求L2范数，默认Frobenius范数计算方法是将矩阵中所有元素的平方和开平方。即计算所有邻居原子j到中心原子i的距离。(53,)\n",
    "            len_nn = keys_tensor.shape[0] #44，中心原子的邻居原子数\n",
    "            # print(index_first, len_nn)\n",
    "            edge_idx_source.extend([index_first] * len_nn) #在edge_idx_source列表末尾扩展序列元素，每个序列元素是每个中心原子的邻居列表[0,...,0,...,35,...,35]，(2016,)\n",
    "            edge_idx_target.extend(keys_tensor[:, 4].tolist()) #将每个中心原子的邻居原子j扩展存入edge_idx_target中，大小是(2016,)的一维列表。\n",
    "            #extend方法是将传入的可迭代对象中的元素逐个添加到列表的末尾。如果传入的是一个列表，则列表中的每个元素都会被添加到原列表中。\n",
    "            edge_fea_single = torch.cat([dist.view(-1, 1), cart_coord.view(1, 3).expand(len_nn, 3)], dim=-1) #53*1 cat #将cart_coord复制扩展为：53*3，得到53*4，单个中心原子的边特征是每个邻居原子到中心原子的距离+该中心原子的坐标\n",
    "            edge_fea_single = torch.cat([edge_fea_single, cart_coords_j, cart_coords[keys_tensor[:, 4]]], dim=-1) #把所有邻居原子的坐标及其在第一晶胞内的等价原子坐标，都拼接在一起，53*10，每个邻居原子到中心原子的距离+该中心原子的坐标（相同的中心原子）+邻居原子的实际坐标+邻居原子在第一晶胞内的等效坐标\n",
    "            edge_fea.append(edge_fea_single) #作为其中一个中心原子与其所有邻居原子的边特征，添加到总的边特征列表edge_fea中。36*53（邻居原子数可能不同）*10\n",
    "\n",
    "            if if_lcmp_graph: #True\n",
    "                #把每个原子i的截断半径内的邻居原子j的列表存入原子索引连接列表中。共72个原子，每个原子的所有邻居原子索引的tensor列表\n",
    "                atom_idx_connect.append(keys_tensor[:, 4]) \n",
    "                #每一个原子及其截断半径内所有邻居原子的边索引列表[range(0, 37), range(37, 74), range(74, 111), ...,]\n",
    "                edge_idx_connect.append(range(edge_idx_connect_cursor, edge_idx_connect_cursor + len_nn)) \n",
    "                edge_idx_connect_cursor += len_nn #统计晶胞中所有原子一共有多少个邻居原子和连接的边（包含超胞中相邻晶胞的原子）\n",
    "\n",
    "        edge_fea = torch.cat(edge_fea).type(default_dtype_torch) #torch.Size([2664, 10])，即这个晶体结构中一共有2016条边\n",
    "        edge_idx = torch.stack([torch.LongTensor(edge_idx_source), torch.LongTensor(edge_idx_target)]) #torch.Size([2, 2664])，每列是1个中心原子到其某个邻居原子\n",
    "\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "\n",
    "    if tb_folder is not None: #训练时：1-600，每次处理一个文件夹；预测时只有一个文件0。inference/C_nanotube_140\n",
    "        if target == 'E_ij':\n",
    "            read_file_list = ['E_ij.h5', 'E_delta_ee_ij.h5', 'E_xc_ij.h5']\n",
    "            graph_key_list = ['E_ij', 'E_delta_ee_ij', 'E_xc_ij']\n",
    "            read_terms_dict = {}\n",
    "            for read_file, graph_key in zip(read_file_list, graph_key_list):\n",
    "                read_terms = {}\n",
    "                fid = h5py.File(os.path.join(tb_folder, read_file), 'r')\n",
    "                for k, v in fid.items():\n",
    "                    key = json.loads(k)\n",
    "                    key = (key[0], key[1], key[2], key[3] - 1, key[4] - 1)\n",
    "                    read_terms[key] = torch.tensor(v[...], dtype=default_dtype_torch)\n",
    "                read_terms_dict[graph_key] = read_terms\n",
    "                fid.close()\n",
    "\n",
    "            local_rotation_dict = {}\n",
    "            if if_require_grad:\n",
    "                fid = fid_rc\n",
    "            else:\n",
    "                fid = h5py.File(os.path.join(tb_folder, 'rc.h5'), 'r')\n",
    "            for k, v in fid.items():\n",
    "                key = json.loads(k)\n",
    "                key = (key[0], key[1], key[2], key[3] - 1, key[4] - 1)  # (R, i, j) i and j is 0-based index\n",
    "                if if_require_grad:\n",
    "                    local_rotation_dict[key] = v\n",
    "                else:\n",
    "                    local_rotation_dict[key] = torch.tensor(v, dtype=default_dtype_torch)\n",
    "            if not if_require_grad:\n",
    "                fid.close()\n",
    "        elif target == 'E_i':\n",
    "            read_file_list = ['E_i.h5']\n",
    "            graph_key_list = ['E_i']\n",
    "            read_terms_dict = {}\n",
    "            for read_file, graph_key in zip(read_file_list, graph_key_list):\n",
    "                read_terms = {}\n",
    "                fid = h5py.File(os.path.join(tb_folder, read_file), 'r')\n",
    "                for k, v in fid.items():\n",
    "                    index_i = int(k)  # index_i is 0-based index\n",
    "                    read_terms[index_i] = torch.tensor(v[...], dtype=default_dtype_torch)\n",
    "                fid.close()\n",
    "                read_terms_dict[graph_key] = read_terms\n",
    "        else:\n",
    "            if data_format == 'h5' or data_format == 'h5_rc_only': #训练时是h5，预测时是h5_rc_only\n",
    "                atom_num_orbital = np.loadtxt(os.path.join(tb_folder, 'num_orbital_per_atom.dat')).astype(int)\n",
    "                if data_format == 'h5':\n",
    "                    with open(os.path.join(tb_folder, 'info.json'), 'r') as info_f:\n",
    "                        info_dict = json.load(info_f)\n",
    "                        spinful = info_dict[\"isspinful\"] #False\n",
    "\n",
    "                if data_format == 'h5':\n",
    "                    if target == 'hamiltonian':\n",
    "                        read_file_list = ['rh.h5']\n",
    "                        graph_key_list = ['term_real']\n",
    "                    elif target == 'phiVdphi':\n",
    "                        read_file_list = ['rphiVdphi.h5']\n",
    "                        graph_key_list = ['term_real']\n",
    "                    elif target == 'density_matrix':\n",
    "                        read_file_list = ['rdm.h5']\n",
    "                        graph_key_list = ['term_real']\n",
    "                    elif target == 'O_ij':\n",
    "                        read_file_list = ['rh.h5', 'rdm.h5', 'rvna.h5', 'rvdee.h5', 'rvxc.h5']\n",
    "                        graph_key_list = ['rh', 'rdm', 'rvna', 'rvdee', 'rvxc']\n",
    "                    else:\n",
    "                        raise ValueError('Unknown prediction target: {}'.format(target))\n",
    "                    read_terms_dict = {}\n",
    "                    for read_file, graph_key in zip(read_file_list, graph_key_list): #这个for循环只执行了一次，分别为rh.h5和term_real\n",
    "                        read_terms = {}\n",
    "                        fid = h5py.File(os.path.join(tb_folder, read_file), 'r') #读取截断半径内局域坐标下旋转后的哈密顿量矩阵,<HDF5 file \"rh.h5\" (mode r)>\n",
    "                        for k, v in fid.items():\n",
    "                            key = json.loads(k)\n",
    "                            key = (key[0], key[1], key[2], key[3], key[4]) #(-1, -1, 0, 0, 14)。v[...]是numpy类型的9*9的局域坐标下旋转后的哈密顿量矩阵\n",
    "                            #v是<HDF5 dataset \"[-1, -1, 0, 1, 15]\": shape (9, 9), type \"<f8\">\n",
    "                            if spinful:\n",
    "                                num_orbital_row = atom_num_orbital[key[3]]\n",
    "                                num_orbital_column = atom_num_orbital[key[4]]\n",
    "                                # soc block order:\n",
    "                                # 1 3\n",
    "                                # 4 2\n",
    "                                if target == 'phiVdphi':\n",
    "                                    raise NotImplementedError\n",
    "                                else:\n",
    "                                    read_value = torch.stack([\n",
    "                                        torch.tensor(v[:num_orbital_row, :num_orbital_column].real, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[:num_orbital_row, :num_orbital_column].imag, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[num_orbital_row:, num_orbital_column:].real, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[num_orbital_row:, num_orbital_column:].imag, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[:num_orbital_row, num_orbital_column:].real, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[:num_orbital_row, num_orbital_column:].imag, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[num_orbital_row:, :num_orbital_column].real, dtype=default_dtype_torch),\n",
    "                                        torch.tensor(v[num_orbital_row:, :num_orbital_column].imag, dtype=default_dtype_torch)\n",
    "                                    ], dim=-1)\n",
    "                                read_terms[key] = read_value\n",
    "                            else:\n",
    "                                read_terms[key] = torch.tensor(v[...], dtype=default_dtype_torch)#将截断半径内局域坐标下旋转后的哈密顿量key和value存入read_terms字典中。在HDF5中，v[...]表示对数据集或数组 v 进行全体索引或切片操作。这种语法意味着选择所有元素或对整个数据集执行操作。\n",
    "                        read_terms_dict[graph_key] = read_terms #某个晶体结构图数据的键为term_real，值为read_terms，read_terms是包含key和value为旋转后的哈密顿量矩阵的字典，即read_terms是图中所有截断半径内局域坐标下的原子对key和旋转后的哈密顿量矩阵v\n",
    "                        fid.close()\n",
    "               \n",
    "                local_rotation_dict = {}\n",
    "                if if_require_grad: #False\n",
    "                    fid = fid_rc\n",
    "                else:\n",
    "                    fid = h5py.File(os.path.join(tb_folder, 'rc.h5'), 'r') #得到截断半径内的排序后的原子对的3*3的单位局域坐标\n",
    "                for k, v in fid.items():\n",
    "                    key = json.loads(k)\n",
    "                    key = (key[0], key[1], key[2], key[3], key[4])  # (R, i, j) i and j is 0-based index\n",
    "                    if if_require_grad: #False\n",
    "                        local_rotation_dict[key] = v\n",
    "                    else:\n",
    "                        local_rotation_dict[key] = torch.tensor(v[...], dtype=default_dtype_torch) #存入截断半径内的排序后的原子对的key和3*3的单位局域坐标value到local_rotation_dict中\n",
    "                if not if_require_grad:\n",
    "                    fid.close()\n",
    "                #read_terms_dict存放的是旋转后的原子间的哈密顿量矩阵，local_rotation_dict存放的是截断半径内的单位坐标系\n",
    "                max_num_orbital = max(atom_num_orbital) #返回指定轨道列表中最大值的元素9/13.\n",
    "\n",
    "            elif data_format == 'npz' or data_format == 'npz_rc_only':\n",
    "                spinful = False\n",
    "                \n",
    "                atom_num_orbital = np.loadtxt(os.path.join(tb_folder, 'num_orbital_per_atom.dat')).astype(int)\n",
    "                if data_format == 'npz':\n",
    "                    graph_key_list = ['term_real']\n",
    "                    read_terms_dict = {'term_real': {}}\n",
    "                    hopping_dict_read = np.load(os.path.join(tb_folder, 'rh.npz'))\n",
    "                    for k, v in hopping_dict_read.items():\n",
    "                        key = json.loads(k)\n",
    "                        key = (key[0], key[1], key[2], key[3] - 1, key[4] - 1)  # (R, i, j) i and j is 0-based index\n",
    "                        read_terms_dict['term_real'][key] = torch.tensor(v, dtype=default_dtype_torch)\n",
    "\n",
    "                local_rotation_dict = {}\n",
    "                local_rotation_dict_read = np.load(os.path.join(tb_folder, 'rc.npz'))\n",
    "                for k, v in local_rotation_dict_read.items():\n",
    "                    key = json.loads(k)\n",
    "                    key = (key[0], key[1], key[2], key[3] - 1, key[4] - 1)\n",
    "                    local_rotation_dict[key] = torch.tensor(v, dtype=default_dtype_torch)\n",
    "\n",
    "                max_num_orbital = max(atom_num_orbital)\n",
    "            else:\n",
    "                raise ValueError(f'Unknown data format: {data_format}')\n",
    "\n",
    "        if target == 'E_i':\n",
    "            term_dict = {}\n",
    "            onsite_term_dict = {}\n",
    "            for graph_key in graph_key_list:\n",
    "                term_dict[graph_key] = torch.full([numbers.shape[0], 1], np.nan, dtype=default_dtype_torch)\n",
    "            for index_atom in range(numbers.shape[0]):\n",
    "                assert index_atom in read_terms_dict[graph_key_list[0]]\n",
    "                for graph_key in graph_key_list:\n",
    "                    term_dict[graph_key][index_atom] = read_terms_dict[graph_key][index_atom]\n",
    "            subgraph = None\n",
    "        else:\n",
    "            if data_format == 'h5_rc_only' or data_format == 'npz_rc_only':\n",
    "                local_rotation = []\n",
    "            else:\n",
    "                term_dict = {}\n",
    "                onsite_term_dict = {}\n",
    "                if target == 'E_ij':\n",
    "                    for graph_key in graph_key_list: #['term_real']\n",
    "                        term_dict[graph_key] = torch.full([edge_fea.shape[0], 1], np.nan, dtype=default_dtype_torch) #edge_fea的size为[2016, 10]，创建了一个给定形状[2016,1]和类型的张量（Tensor），其中所有元素都被初始化为 NaN（Not a Number，非数值）。\n",
    "                    local_rotation = []\n",
    "                    if separate_onsite is True:\n",
    "                        for graph_key in graph_key_list:\n",
    "                            onsite_term_dict['onsite_' + graph_key] = torch.full([numbers.shape[0], 1], np.nan, dtype=default_dtype_torch)\n",
    "                else:\n",
    "                    term_mask = torch.zeros(edge_fea.shape[0], dtype=torch.bool) #edge_fea：torch.Size([2664, 10])，生成[2664个False]一维张量\n",
    "                    for graph_key in graph_key_list:  #['term_real']，次for循环只执行一次\n",
    "                        if spinful:\n",
    "                            term_dict[graph_key] = torch.full([edge_fea.shape[0], max_num_orbital, max_num_orbital, 8],\n",
    "                                                              np.nan, dtype=default_dtype_torch)\n",
    "                        else:\n",
    "                            if target == 'phiVdphi':\n",
    "                                term_dict[graph_key] = torch.full([edge_fea.shape[0], max_num_orbital, max_num_orbital, 3],\n",
    "                                                                  np.nan, dtype=default_dtype_torch)\n",
    "                            else:\n",
    "                                term_dict[graph_key] = torch.full([edge_fea.shape[0], max_num_orbital, max_num_orbital],\n",
    "                                                                  np.nan, dtype=default_dtype_torch) #用np.nan填充size形状的张量，数据类型为default_dtype_torch，即生成用于存放第一晶胞内所有中心原子与其邻居原子的所有2016个边的9*9哈密顿量矩阵，其中所有元素都被初始化为 NaN（Not a Number，非数值）。\n",
    "                        # print(term_dict[graph_key].shape) #torch.Size([2664, 9, 9])\n",
    "\n",
    "                    local_rotation = []\n",
    "                    if separate_onsite is True:\n",
    "                        for graph_key in graph_key_list:\n",
    "                            if spinful:\n",
    "                                onsite_term_dict['onsite_' + graph_key] = torch.full(\n",
    "                                    [numbers.shape[0], max_num_orbital, max_num_orbital, 8],\n",
    "                                    np.nan, dtype=default_dtype_torch)\n",
    "                            else:\n",
    "                                if target == 'phiVdphi':\n",
    "                                    onsite_term_dict['onsite_' + graph_key] = torch.full(\n",
    "                                        [numbers.shape[0], max_num_orbital, max_num_orbital, 3],\n",
    "                                        np.nan, dtype=default_dtype_torch)\n",
    "                                else:\n",
    "                                    onsite_term_dict['onsite_' + graph_key] = torch.full(\n",
    "                                        [numbers.shape[0], max_num_orbital, max_num_orbital],\n",
    "                                        np.nan, dtype=default_dtype_torch)\n",
    "\n",
    "            # read_terms_dict存放的是旋转后的原子间的哈密顿量矩阵，local_rotation_dict存放的是截断半径内的边的旋转矩阵\n",
    "            inv_lattice = torch.inverse(lattice).type(default_dtype_torch) #晶格矢量求逆\n",
    "            for index_edge in range(edge_fea.shape[0]):  #edge_fea.shape=2016*10, edge_idx.shape=2*2016, index_edge从0-2015\n",
    "                # h_{i0, jR} i and j is 0-based index，用于将笛卡尔坐标系中的原子位置转换到倒格矢空间中。torch.round把输入张量的每个元素舍入到最近的整数。\n",
    "                R = torch.round(edge_fea[index_edge, 4:7].cpu() @ inv_lattice - edge_fea[index_edge, 7:10].cpu() @ inv_lattice).int().tolist() #求该实际邻居原子j的晶胞索引，R*inv(R)=E\n",
    "                #把每个邻居原子的实际坐标*晶格矢量的逆 - 等效的第一晶胞内的邻居原子坐标*晶格矢量的逆，R表示了在晶格逆变换下，两个给定向量位置差的整数近似值。\n",
    "                i, j = edge_idx[:, index_edge] #每一个边对应的原子i及其邻居原子j\n",
    "\n",
    "                key_term = (*R, i.item(), j.item()) #每一条边对应的中心原子i和实际邻居原子j的哈密顿量的key\n",
    "                if data_format == 'h5_rc_only' or data_format == 'npz_rc_only':\n",
    "                    local_rotation.append(local_rotation_dict[key_term]) #取出key_term对应的中心原子的单位局域坐标系，即取出所有中心原子i的局域坐标系\n",
    "                else:\n",
    "                    if key_term in read_terms_dict[graph_key_list[0]]: #读取晶体图数据中对应'term_real'键下的k,v，判断key_term是否在k中。\n",
    "                        for graph_key in graph_key_list: #'term_real'\n",
    "                            if target == 'E_ij':\n",
    "                                term_dict[graph_key][index_edge] = read_terms_dict[graph_key][key_term]\n",
    "                            else:\n",
    "                                term_mask[index_edge] = True #标志着第index_edge个边的哈密顿量矩阵被转存到了term_dict中。[2664个False依次变为True]\n",
    "                                if spinful:\n",
    "                                    term_dict[graph_key][index_edge, :atom_num_orbital[i], :atom_num_orbital[j], :] = read_terms_dict[graph_key][key_term]\n",
    "                                else:\n",
    "                                    term_dict[graph_key][index_edge, :atom_num_orbital[i], :atom_num_orbital[j]] = read_terms_dict[graph_key][key_term] \n",
    "                                    #将read_terms_dict键下key_term键下的9*9哈密顿量矩阵保存到term_dict键下的1*9*9的哈密顿量矩阵，遍历所有index_edge等使得term_dict中key为term_real，value为2016*9*9，2016条边，每条边对应一个9*9的小哈密顿量矩阵\n",
    "                        local_rotation.append(local_rotation_dict[key_term]) #读取每个原子的截断半径内的排序后的3*3的单位局域坐标，存放到局域旋转列表中。\n",
    "                    else:\n",
    "                        raise NotImplementedError(\n",
    "                            \"Not yet have support for graph radius including hopping without calculation\")\n",
    "            # term_dict存放的是不同key_term下旋转后的原子间的哈密顿量矩阵，local_rotation存放的是不同key_term下截断半径内的旋转矩阵\n",
    "            # term_mask 为2664个True\n",
    "\n",
    "            if separate_onsite is True and data_format != 'h5_rc_only' and data_format != 'npz_rc_only':\n",
    "                for index_atom in range(numbers.shape[0]):\n",
    "                    key_term = (0, 0, 0, index_atom, index_atom)\n",
    "                    assert key_term in read_terms_dict[graph_key_list[0]]\n",
    "                    for graph_key in graph_key_list:\n",
    "                        if target == 'E_ij':\n",
    "                            onsite_term_dict['onsite_' + graph_key][index_atom] = read_terms_dict[graph_key][key_term]\n",
    "                        else:\n",
    "                            if spinful:\n",
    "                                onsite_term_dict['onsite_' + graph_key][index_atom, :atom_num_orbital[i], :atom_num_orbital[j], :] = \\\n",
    "                                read_terms_dict[graph_key][key_term]\n",
    "                            else:\n",
    "                                onsite_term_dict['onsite_' + graph_key][index_atom, :atom_num_orbital[i], :atom_num_orbital[j]] = \\\n",
    "                                read_terms_dict[graph_key][key_term]\n",
    "\n",
    "            if if_lcmp_graph: #True\n",
    "                local_rotation = torch.stack(local_rotation, dim=0) #2664*3*3\n",
    "                assert local_rotation.shape[0] == edge_fea.shape[0] #都是2664\n",
    "                r_vec = edge_fea[:, 1:4] - edge_fea[:, 4:7] #edge_fea[:, 1:4]是中心原子i的坐标，edge_fea[:, 4:7]是相邻原子j的实际坐标，2016*3原子之间的向量差\n",
    "                r_vec = r_vec.unsqueeze(1) #在输入张量的特定维度上增加一个维度。torch.Size([2664, 1, 3])，可以用来改变张量的形状和结构。2016*1*3，即原子间向量的集合。\n",
    "\n",
    "                if huge_structure is False:\n",
    "                    #torch.Size([2664, 1, 1, 3])，torch.Size([1, 2664, 3, 3]) = torch.Size([2664, 2664, 1, 3])=torch.Size([7096896, 3])\n",
    "                    r_vec = torch.matmul(r_vec[:, None, :, :], local_rotation[None, :, :, :].to(r_vec.device)).reshape(-1, 3) \n",
    "                    #r_vec为所有不同局域坐标下的原子间距离向量的集合。把所有边都转为不同局域坐标系下的向量。\n",
    "                    \n",
    "                    if if_new_sp: #False\n",
    "                        r_vec = torch.nn.functional.normalize(r_vec, dim=-1)\n",
    "                        angular_expansion = _spherical_harmonics(num_l - 1, -r_vec[..., 2], r_vec[..., 0],\n",
    "                                                                 r_vec[..., 1])\n",
    "                        angular_expansion.mul_(torch.cat([\n",
    "                            (math.sqrt(2 * l + 1) / math.sqrt(4 * math.pi)) * torch.ones(2 * l + 1,\n",
    "                                                                                         dtype=angular_expansion.dtype,\n",
    "                                                                                         device=angular_expansion.device)\n",
    "                            for l in range(num_l)\n",
    "                        ]))\n",
    "                        angular_expansion = angular_expansion.reshape(edge_fea.shape[0], edge_fea.shape[0], -1)\n",
    "                    else:\n",
    "                        r_vec_sp = get_spherical_from_cartesian(r_vec) #将所有不同局域坐标下的原子间距离向量（笛卡尔坐标）的集合，计算出球坐标系下的theta角和phi角。torch.Size([4064256, 2])\n",
    "                        sph_harm_func = SphericalHarmonics()\n",
    "                        angular_expansion = []\n",
    "                        for l in range(num_l): #num_l=0-4\n",
    "                            angular_expansion.append(sph_harm_func.get(l, r_vec_sp[:, 0], r_vec_sp[:, 1])) #sph_harm_func.get返回tensor of shape [*theta.shape, 2*l+1]，涉及球谐函数，这里没搞懂。这个方法返回的结果是针对每个角度值(θ, φ)对应的球谐基函数展开，随后这些结果被追加到angular_expansion数组中。\n",
    "                        angular_expansion = torch.cat(angular_expansion, dim=-1).reshape(edge_fea.shape[0], edge_fea.shape[0], -1) #torch.Size([2664, 2664, 16]，计算每个边在不同局域坐标系下的角度(θ, φ)的球谐基函数展开，这对于许多模拟和计算任务来说是非常有用的。这样的角度展开允许你在后续的计算中方便地使用球面函数的性质，比如在分子动力学模拟、光照计算或声场模拟中。\n",
    "                subgraph_atom_idx_list = []\n",
    "                subgraph_edge_idx_list = []\n",
    "                subgraph_edge_ang_list = []\n",
    "                subgraph_index = []\n",
    "                index_cursor = 0\n",
    "\n",
    "                for index in range(edge_fea.shape[0]):#0-2664\n",
    "                    # h_{i0, jR}\n",
    "                    i, j = edge_idx[:, index] #edge_idx尺寸torch.Size([2, 2016])，遍历每一个原子对\n",
    "                    subgraph_atom_idx = torch.stack([i.repeat(len(atom_idx_connect[i])), atom_idx_connect[i]]).T #atom_idx_connect[i]#是第i个原子的截断半径内的所有邻居原子j的列表。torch.tensor.repeat()函数可以对张量进行重复扩充。原子i和每一个邻居原子j的对的转置，转置后是torch.Size([53, 2])。\n",
    "                    subgraph_edge_idx = torch.LongTensor(edge_idx_connect[i]) #torch.Size([53])#取出第i个中心原子i的截断半径内的所有邻居原子的边索引\n",
    "                    # print(subgraph_atom_idx.shape) #torch.Size([37, 2])\n",
    "                    # print(subgraph_edge_idx.shape) #torch.Size([37])\n",
    "\n",
    "                    if huge_structure:\n",
    "                        r_vec_tmp = torch.matmul(r_vec[subgraph_edge_idx, :, :], local_rotation[index, :, :].to(r_vec.device)).reshape(-1, 3) #将全局坐标下的边向量r_vec，转为在各自单位局域坐标系下的边向量r_vec_tmp，53×3。\n",
    "                        if if_new_sp:\n",
    "                            r_vec_tmp = torch.nn.functional.normalize(r_vec_tmp, dim=-1)\n",
    "                            subgraph_edge_ang = _spherical_harmonics(num_l - 1, -r_vec_tmp[..., 2], r_vec_tmp[..., 0], r_vec_tmp[..., 1])\n",
    "                            subgraph_edge_ang.mul_(torch.cat([\n",
    "                                (math.sqrt(2 * l + 1) / math.sqrt(4 * math.pi)) * torch.ones(2 * l + 1,\n",
    "                                                                                             dtype=subgraph_edge_ang.dtype,\n",
    "                                                                                             device=subgraph_edge_ang.device)\n",
    "                                for l in range(num_l)\n",
    "                            ]))\n",
    "                        else:\n",
    "                            r_vec_sp = get_spherical_from_cartesian(r_vec_tmp) #获得邻居原子局域坐标下的alpha和beta角度，53*2\n",
    "                            sph_harm_func = SphericalHarmonics()\n",
    "                            angular_expansion = []\n",
    "                            for l in range(num_l):\n",
    "                                angular_expansion.append(sph_harm_func.get(l, r_vec_sp[:, 0], r_vec_sp[:, 1])) #25*53*1\n",
    "                            subgraph_edge_ang = torch.cat(angular_expansion, dim=-1).reshape(-1, num_l ** 2) #53*25\n",
    "                    else:\n",
    "                        subgraph_edge_ang = angular_expansion[subgraph_edge_idx, index, :] #取出第i个原子的截断半径内的所有邻居原子的边的球谐基函数展开，torch.Size([53, 1, 25])\n",
    "                    subgraph_atom_idx_list.append(subgraph_atom_idx) #每一条边的一个原子i及其截断半径内的所有邻居原子j的索引作为一个子图节点集合，存入subgraph_atom_idx_list中，2016*53*2。\n",
    "                    subgraph_edge_idx_list.append(subgraph_edge_idx) #每一条边的一个原子i及其与所有邻居原子j的边索引作为一个子图的边集合，存入subgraph_edge_idx_list中，2016*53。\n",
    "                    subgraph_edge_ang_list.append(subgraph_edge_ang) #每一条边的一个原子i及其所有邻居原子j的边的球谐函数展开作为一个子图的边角度集合，存入subgraph_edge_ang_list中，2016*53*25。\n",
    "                    subgraph_index += [index_cursor] * len(atom_idx_connect[i]) #每一条边的一个原子i的邻居原子个数的索引游标[53个0,53个1,...,53个2016]相拼接在一个列表中，共2016*53个子图的索引列表。\n",
    "                    index_cursor += 1\n",
    "\n",
    "                    subgraph_atom_idx = torch.stack([j.repeat(len(atom_idx_connect[j])), atom_idx_connect[j]]).T #atom_idx_connect[j]是第j个原子的邻居原子的列表。原子j和每一个邻居原子的对的转置，torch.Size([53, 2])\n",
    "                    subgraph_edge_idx = torch.LongTensor(edge_idx_connect[j]) #torch.Size([53])#取出原子j的所有邻居原子的边\n",
    "                    if huge_structure:\n",
    "                        r_vec_tmp = torch.matmul(r_vec[subgraph_edge_idx, :, :], local_rotation[index, :, :].to(r_vec.device)).reshape(-1, 3)\n",
    "                        if if_new_sp:\n",
    "                            r_vec_tmp = torch.nn.functional.normalize(r_vec_tmp, dim=-1)\n",
    "                            subgraph_edge_ang = _spherical_harmonics(num_l - 1, -r_vec_tmp[..., 2], r_vec_tmp[..., 0], r_vec_tmp[..., 1])\n",
    "                            subgraph_edge_ang.mul_(torch.cat([\n",
    "                                (math.sqrt(2 * l + 1) / math.sqrt(4 * math.pi)) * torch.ones(2 * l + 1,\n",
    "                                                                                             dtype=subgraph_edge_ang.dtype,\n",
    "                                                                                             device=subgraph_edge_ang.device)\n",
    "                                for l in range(num_l)\n",
    "                            ]))\n",
    "                        else:\n",
    "                            r_vec_sp = get_spherical_from_cartesian(r_vec_tmp)\n",
    "                            sph_harm_func = SphericalHarmonics()\n",
    "                            angular_expansion = []\n",
    "                            for l in range(num_l):\n",
    "                                angular_expansion.append(sph_harm_func.get(l, r_vec_sp[:, 0], r_vec_sp[:, 1]))\n",
    "                            subgraph_edge_ang = torch.cat(angular_expansion, dim=-1).reshape(-1, num_l ** 2)#取出原子j的所有邻居原子的边的角度球谐函数，torch.Size([53,25])\n",
    "                    else:\n",
    "                        subgraph_edge_ang = angular_expansion[subgraph_edge_idx, index, :] #53*1*25\n",
    "                    subgraph_atom_idx_list.append(subgraph_atom_idx) #每一条边的另一个原子j及其截断半径内的所有邻居原子的索引作为一个子图节点集合，存入subgraph_atom_idx_list中。里面已经包含了之前原子i的所有邻居原子索引，2*2016*53*2\n",
    "                    subgraph_edge_idx_list.append(subgraph_edge_idx) #每一条边的另一个原子j及其截断半径内的所有邻居原子的边作为一个子图的边集合，存入subgraph_edge_idx_list中，2*2016*53。\n",
    "                    subgraph_edge_ang_list.append(subgraph_edge_ang) #每一条边的另一个原子j及其截断半径内的所有邻居原子的边的球谐函数作为一个子图的边角度集合，存入subgraph_edge_ang_list中2*2016*53*25。\n",
    "                    subgraph_index += [index_cursor] * len(atom_idx_connect[j]) #每一条边的另一个原子j的截断半径内的邻居原子个数的索引游标[53个2017,53个2018,...,53个4032]相拼接在一个列表中，共2*2016*53个子图的列表。\n",
    "                    index_cursor += 1\n",
    "                #这个subgraph是由所有边的各自两个原子的各自邻居原子和边构成的子图\n",
    "                subgraph =  {\"subgraph_atom_idx\":torch.cat(subgraph_atom_idx_list, dim=0), #torch.Size([197136, 2])\n",
    "                             \"subgraph_edge_idx\":torch.cat(subgraph_edge_idx_list, dim=0), #torch.Size([197136])\n",
    "                             \"subgraph_edge_ang\":torch.cat(subgraph_edge_ang_list, dim=0), #torch.Size([197136, 16])\n",
    "                             \"subgraph_index\":torch.LongTensor(subgraph_index)} #torch.Size([197136])\n",
    "                # print(\"\\n\")\n",
    "                # print(\"subgraph_atom_idx.shape = \", subgraph[\"subgraph_atom_idx\"].shape)\n",
    "                # print(\"subgraph_edge_idx.shape = \", subgraph[\"subgraph_edge_idx\"].shape)\n",
    "                # print(\"subgraph_edge_ang.shape = \", subgraph[\"subgraph_edge_ang\"].shape)\n",
    "                # print(\"subgraph_index.shape = \", subgraph[\"subgraph_index\"].shape)\n",
    "            else:\n",
    "                subgraph = None\n",
    "\n",
    "        if data_format == 'h5_rc_only' or data_format == 'npz_rc_only':\n",
    "            # 创建一个新的空图\n",
    "            import networkx as nx\n",
    "            G = nx.Graph()\n",
    "            # 添加节点及其属性：原子特征和位置坐标\n",
    "            for i, (x, coords) in enumerate(zip(numbers, cart_coords)):\n",
    "                G.add_node(i, x=x.item(), position=coords.numpy())\n",
    "            # 添加边及其属性：边的特征\n",
    "            for i, (src, dst) in enumerate(edge_idx.t()):\n",
    "                G.add_edge(src.item(), dst.item(), attr=edge_fea[i].numpy())\n",
    "            # 添加图的整体属性：晶格矢量\n",
    "            G.graph['lattice'] = lattice.numpy()\n",
    "            # 打印图的信息以确认转换成功\n",
    "            # print(\"number_of_nodes = \", G.number_of_nodes()) #72\n",
    "            # print(\"number_of_edges = \", G.number_of_edges()) #1368\n",
    "            from .from_graphormer.functional import shortest_path_distance, cal_voronoi_and_centrality\n",
    "            node_paths, edge_paths = shortest_path_distance(G, shortest_path_length)\n",
    "            # print(node_paths.shape) #torch.Size([1, 72, 72, 5])\n",
    "            # print(edge_paths.shape) #torch.Size([1, 72, 72, 4])\n",
    "            voronoi_values, centralities = cal_voronoi_and_centrality(cart_coords, lattice, material_dimension)\n",
    "\n",
    "            data = Data(x=numbers, edge_index=edge_idx, edge_attr=edge_fea, stru_id=stru_id, cart_coords=cart_coords, lattice=lattice,\n",
    "                        voronoi_values=voronoi_values, centralities=centralities, node_paths=node_paths, edge_paths=edge_paths,\n",
    "                        term_mask=None, term_real=None, onsite_term_real=None,\n",
    "                        atom_num_orbital=torch.tensor(atom_num_orbital),\n",
    "                        subgraph_dict=subgraph,\n",
    "                        **kwargs)\n",
    "            #numbers是[原子个数*0]；edge_idx是边索引[2,2016];edge_fea是节点特征[2016,10];stru_id是0；term_mask=None；#term_dict=None；onsite_term_dict=None；atom_num_orbital是[预测的结构C原子个数*13]；subgraph是截断半径内的子图,包含了子图中原子对、边、球谐函数展开和子图索引的信息。\n",
    "        else:\n",
    "            if target == 'E_ij' or target == 'E_i':\n",
    "                data = Data(x=numbers, edge_index=edge_idx, edge_attr=edge_fea, stru_id=stru_id, cart_coords=cart_coords, lattice=lattice,\n",
    "                            **term_dict, **onsite_term_dict,\n",
    "                            subgraph_dict=subgraph,\n",
    "                            spinful=False,\n",
    "                            **kwargs)\n",
    "            else:\n",
    "                # 创建一个新的空图\n",
    "                import networkx as nx\n",
    "                G = nx.Graph()\n",
    "                # 添加节点及其属性：原子特征和位置坐标\n",
    "                for i, (x, coords) in enumerate(zip(numbers, cart_coords)):\n",
    "                    G.add_node(i, x=x.item(), position=coords.numpy())\n",
    "                # 添加边及其属性：边的特征\n",
    "                for i, (src, dst) in enumerate(edge_idx.t()):\n",
    "                    G.add_edge(src.item(), dst.item(), attr=edge_fea[i].numpy())\n",
    "                # 添加图的整体属性：晶格矢量\n",
    "                G.graph['lattice'] = lattice.numpy()\n",
    "                # 打印图的信息以确认转换成功\n",
    "                # print(\"number_of_nodes = \", G.number_of_nodes()) #72\n",
    "                # print(\"number_of_edges = \", G.number_of_edges()) #1368\n",
    "\n",
    "                from from_graphormer.functional import shortest_path_distance, cal_voronoi_and_centrality\n",
    "                node_paths, edge_paths = shortest_path_distance(G, shortest_path_length)\n",
    "                # print(node_paths.shape) #torch.Size([1, 72, 72, 5])\n",
    "                # print(edge_paths.shape) #torch.Size([1, 72, 72, 4])\n",
    "\n",
    "                voronoi_values, centralities = cal_voronoi_and_centrality(cart_coords, lattice, material_dimension)\n",
    "                # print(voronoi_values.shape)\n",
    "                # print(centralities.shape)\n",
    "                # numbers是原子序数72个6列表；edge_idx是边索引[2,2016];edge_fea是节点特征[2016,10];stru_id是晶体结构图id；term_mask是[2016个True]张量；#term_dict是key为term_real，value为2016*9*9旋转后的哈密顿量；\n",
    "                # onsite_term_dict是{}；atom_num_orbital是[72个13的列表]即每个原子使用的总轨道数；subgraph是由截断半径内的所有边的各自两个原子的各自邻居原子和边构成的子图,包含了子图中原子对、边、球谐函数展开和子图索引的信息。\n",
    "                data = Data(x=numbers, edge_index=edge_idx, edge_attr=edge_fea, stru_id=stru_id, voronoi_values=voronoi_values, centralities=centralities,\n",
    "                            cart_coords=cart_coords, lattice=lattice, term_mask=term_mask, node_paths=node_paths, edge_paths=edge_paths,\n",
    "                            **term_dict, **onsite_term_dict,\n",
    "                            atom_num_orbital=atom_num_orbital,\n",
    "                            subgraph_dict=subgraph,\n",
    "                            spinful=spinful,\n",
    "                            **kwargs)\n",
    "\n",
    "    else:\n",
    "        data = Data(x=numbers, edge_index=edge_idx, edge_attr=edge_fea, stru_id=stru_id, cart_coords=cart_coords, lattice=lattice, **kwargs)\n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "###以下为调试用，可删除\n",
    "def process_worker(folder, **kwargs):\n",
    "    default_dtype_torch = torch.get_default_dtype()\n",
    "    stru_id = os.path.split(folder)[-1]  # 如果给出的是一个目录和文件名，则输出路径和文件名，如果给出的是一个目录名，则输出路径和为空文件名。0-575\n",
    "\n",
    "    structure = Structure(np.loadtxt(os.path.join(folder, 'lat.dat')),  # 加载第i个结构的晶格向量、原子序数和笛卡尔坐标\n",
    "                          np.loadtxt(os.path.join(folder, 'element.dat')),\n",
    "                          np.loadtxt(os.path.join(folder, 'site_positions.dat')),\n",
    "                          coords_are_cartesian=True,\n",
    "                          to_unit_cell=False)  # 用pymatgen创建结构\n",
    "\n",
    "    cart_coords = torch.tensor(structure.cart_coords, dtype=default_dtype_torch)  # 原子的笛卡尔坐标，当前的默认浮点torch.dtype\n",
    "    frac_coords = torch.tensor(structure.frac_coords, dtype=default_dtype_torch)\n",
    "    numbers = torch.tensor(structure.atomic_numbers)  # List of atomic numbers.\n",
    "    structure.lattice.matrix.setflags(write=True)  # 描述structure.lattice.matrix是否可以写入。\n",
    "    lattice = torch.tensor(structure.lattice.matrix,\n",
    "                           dtype=default_dtype_torch)  # 读取structure.lattice.matrix，并转为default_dtype_torch类型\n",
    "    huge_structure = False  ## r=self.radius==-1。numerical_tol？\n",
    "\n",
    "    return get_graph(cart_coords, frac_coords, numbers, stru_id, radius=7.0, material_dimension=2,\n",
    "                         numerical_tol=1e-8, lattice=lattice, default_dtype_torch=default_dtype_torch,\n",
    "                         tb_folder=folder, data_format=\"h5\", num_l=4, shortest_path_length=5,\n",
    "                         if_lcmp_graph=True, separate_onsite=False, target=\"hamiltonian\", \n",
    "                         huge_structure=huge_structure, if_new_sp=False, **kwargs) #获得图类型的数据集return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder = \"/fs2/home/ndsim10/DeepQT/0_generate_dataset/expand_dataset/processed/1\"\n",
    "    data = process_worker(folder)\n",
    "    print(data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "178ff100-97e4-40d3-b249-0b530be6331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from pymatgen.core.structure import Structure\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from pathos.multiprocessing import ProcessingPool as Pool #多参数输入并行计算\n",
    "\n",
    "from graph import get_graph\n",
    "\n",
    "\n",
    "class HData(InMemoryDataset):\n",
    "    def __init__(self, config: dict, default_dtype_torch, transform=None, pre_transform=None, pre_filter=None): \n",
    "        # 必须传入 config 和 default_dtype_torch，保持你的原始参数风格\n",
    "        if config is None:\n",
    "            raise ValueError(\"config must be provided\")\n",
    "        if default_dtype_torch is None:\n",
    "            raise ValueError(\"default_dtype_torch must be provided\")\n",
    "        \n",
    "        self.processed_data_dir = config['basic']['processed_data_dir']\n",
    "        self.graph_dir = config['basic']['graph_dir']\n",
    "        self.data_format = config['basic']['data_format']\n",
    "        self.target = config['basic']['target']\n",
    "        self.material_dimension = config['basic']['material_dimension']\n",
    "        self.multiprocessing = config['basic']['multiprocessing'] #多进程处理转换为图结构数据\n",
    "        self.radius = config['graph']['radius'] #这个半径和预处理的截断半径不同\n",
    "        self.num_l = config['graph']['num_l'] #球谐函数展开的角量子数\n",
    "        self.if_lcmp_graph = config['graph']['if_lcmp_graph']\n",
    "        self.separate_onsite = config['graph']['separate_onsite']\n",
    "        self.new_sp = config['graph']['new_sp']\n",
    "        self.shortest_path_length = config['graph']['shortest_path_length']\n",
    "        self.default_dtype_torch = default_dtype_torch\n",
    "        self.transform = transform #None\n",
    "        self.pre_transform = pre_transform #None\n",
    "        self.pre_filter = pre_filter #None\n",
    "        self.__indices__ = None\n",
    "        self.__data_list__ = None\n",
    "        self._indices = None\n",
    "        self._data_list = None\n",
    "\n",
    "        if self.if_lcmp_graph: #True\n",
    "            lcmp_str = f'{self.num_l}l'\n",
    "        else:\n",
    "            lcmp_str = 'WithoutLCMP'\n",
    "        if self.separate_onsite is True:\n",
    "            onsite_str = '-SeparateOnsite'\n",
    "        else:\n",
    "            onsite_str = ''\n",
    "        if self.new_sp:\n",
    "            new_sp_str = '-NewSP'\n",
    "        else:\n",
    "            new_sp_str = ''\n",
    "        if self.target == 'hamiltonian':\n",
    "            title = 'HGraph'\n",
    "        else:\n",
    "            raise ValueError('Unknown prediction target: {}'.format(self.target))\n",
    "            \n",
    "        self.graph_file_name = f'{title}-{self.data_format}-{lcmp_str}{onsite_str}{new_sp_str}.pkl'\n",
    "        self.data_file = os.path.join(self.graph_dir, self.graph_file_name) #创建图数据的文件路径和名字\n",
    "        os.makedirs(self.graph_dir, exist_ok=True)\n",
    "        \n",
    "        print(f'Graph data file: {self.graph_file_name}')\n",
    "        if os.path.exists(self.data_file):\n",
    "            print(\"Use existing graph data file\")\n",
    "            self.load()\n",
    "        else:\n",
    "            super(HData, self).__init__(self.processed_data_dir, transform, pre_transform, pre_filter)\n",
    "\n",
    "        \n",
    "        begin = time.time()\n",
    "        try:\n",
    "            loaded_data = self.loaded_data #加载torch.save()保存的模型文件。\n",
    "        except AttributeError:\n",
    "            raise RuntimeError('Error in loading graph data file, try to delete it and generate the graph file with the current version of PyG')\n",
    "        max_element = -1\n",
    "        if len(loaded_data) == 2:\n",
    "            warnings.warn('You are using the graph data file with an old version')\n",
    "            self.data, self.slices = loaded_data\n",
    "            self.info = {\n",
    "                \"spinful\": False,\n",
    "                \"index_to_Z\": torch.arange(max_element + 1), #max_element=-1，创建一个空的张量，因为没有任何数字满足这样的条件。\n",
    "                \"Z_to_index\": torch.arange(max_element + 1),\n",
    "            }\n",
    "        elif len(loaded_data) == 3: #加载新版本图数据,data, slices, dict3个文件\n",
    "            self.data, self.slices, tmp = loaded_data #tmp是dict(spinful=spinful, index_to_Z=index_to_Z, Z_to_index=Z_to_index)。#False, [83], [第83个位置为0，其余为-1的(100,)张量]\n",
    "            if isinstance(tmp, dict): #如果对象tmp的类型与参数二的类型（dict）相同则返回 True，否则返回 False。\n",
    "                self.info = tmp\n",
    "                print(f\"Atomic types: {self.info['index_to_Z'].tolist()}\") #取出去重和排序后的原子序数列表，即不同的原子类型列表，[83]\n",
    "                print(f\"Atomic types vectors: {self.info['Z_to_index'].tolist()}\")#[第83个位置为0，其余为-1的(100,)张量]\n",
    "            else:\n",
    "                warnings.warn('You are using an old version of the graph data file')\n",
    "                self.info = {\n",
    "                    \"spinful\": tmp,\n",
    "                    \"index_to_Z\": torch.arange(max_element + 1), #创建一个空的张量，因为没有任何数字满足这样的条件。\n",
    "                    \"Z_to_index\": torch.arange(max_element + 1),\n",
    "                }\n",
    "        else:\n",
    "            raise RuntimeError(f'Unexpected format in saved graph file: found {len(loaded_data)} elements')\n",
    "            \n",
    "        print(f'Finish loading the processed {len(self)} structures (spinful: {self.info[\"spinful\"]}, '\n",
    "              f'the number of atomic types: {len(self.info[\"index_to_Z\"])}), cost {time.time() - begin:.0f} seconds')\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self): #检查data/processed目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "        pass\n",
    "        return []\n",
    "\n",
    "    def load(self):\n",
    "        # 如果文件存在，就直接加载，不再走 process\n",
    "        if os.path.exists(self.data_file):\n",
    "            print(\"Loading existing dataset:\", self.data_file)\n",
    "            self.loaded_data = torch.load(self.data_file)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Processed file {self.data_file} not found, please run process()\")\n",
    "    \n",
    "    def process(self):\n",
    "        print('Process new data file......')\n",
    "        \n",
    "        begin = time.time()\n",
    "        folder_list = []\n",
    "        for root, dirs, files in os.walk(self.processed_data_dir): #example/work_dir/dataset/processed\n",
    "            if (self.data_format == 'h5' and 'rc.h5' in files) or (\n",
    "                    self.data_format == 'npz' and 'rc.npz' in files):\n",
    "                folder_list.append(root) #example/work_dir/dataset/processed/0-575\n",
    "        folder_list = sorted(folder_list) #按processed中数字文件夹的顺序排序\n",
    "        assert len(folder_list) != 0, \"Can not find any structure\"\n",
    "\n",
    "\n",
    "        if self.multiprocessing == 0:\n",
    "            print(f'Use multiprocessing (nodes = num_processors x num_threads = 1 x {torch.get_num_threads()})') #获得用于并行化CPU操作的OpenMP线程数，然后使用单进程运行。\n",
    "            data_list = [self.process_worker(folder) for folder in tqdm.tqdm(folder_list, ncols=80, leave=False, position=0)] #tqdm.tqdm()实现进度条，输入一个可迭代对象folder_list。输入每一个晶体结构文件夹到process_worker函数中。得到每一个晶体结构的图类型的数据，即一个晶体结构对应一个图data。\n",
    "        else:\n",
    "            pool_dict = {} if self.multiprocessing < 0 else {'nodes': self.multiprocessing} #如果self.multiprocessing>=0，pool_dict被赋值为包含一个键值对'nodes': self.multiprocessing 的字典。\n",
    "            torch_num_threads = torch.get_num_threads()\n",
    "            torch.set_num_threads(1)\n",
    "            #with中代码主要目的是基于条件选择是否使用多进程来并行处理 folder_list 中的任务，并在处理过程中提供了进度条的可视化信息。\n",
    "            with Pool(**pool_dict) as pool: #Pool()创建了一个进程池，参数**pool_dict是将字典内容解包作为关键字参数传递给ProcessingPool。with语句确保在离开其代码块之前正确关闭进程池，这样可以避免资源泄漏。\n",
    "                nodes = pool.nodes #获取进程池的节点数，下面打印出多进程处理的相关信息。\n",
    "                print(f'Use multiprocessing (nodes = num_processors x num_threads = {nodes} x {torch.get_num_threads()})') #处理器数量和线程数量\n",
    "                data_list = list(tqdm.tqdm(pool.imap(self.process_worker, folder_list), total=len(folder_list), ncols=80, leave=False, position=0)) #对folder_list中的每个元素调用self.process_worker函数，使用进程池并行处理，返回各自结构的图数据。tqdm.tqdm则提供了一个可视化的进度条，用于显示处理进度。total=len(folder_list)设置了进度条的总数。\n",
    "            torch.set_num_threads(torch_num_threads) #设置 PyTorch 运行时所使用的线程数\n",
    "\n",
    "        if self.pre_filter is not None: #pre_filter=None\n",
    "            data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "        if self.pre_transform is not None: #pre_transform=None\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "        \n",
    "        index_to_Z, Z_to_index = self.element_statistics(data_list) #[83], [第83个位置为0，其余为-1的张量]。且遍历所有晶体结构，把每个晶体结构的data.x都变为了[36个0或1等]，这里不是显式。\n",
    "        spinful = data_list[0].spinful #False\n",
    "        for d in data_list:\n",
    "            assert spinful == d.spinful\n",
    "\n",
    "        data, slices = self.collate(data_list) #将Data或HeteroData对象列表整理为InMemoryDataset的内部存储格式。\n",
    "        torch.save((data, slices, dict(spinful=spinful, index_to_Z=index_to_Z, Z_to_index=Z_to_index)), self.data_file) #将这些数据对象保存到指定路径的文件中，存data, slices, dict3个文件，以便之后可以通过 torch.load() 函数重新加载并使用这些数据。已存储，近19GB的数据。\n",
    "        print('Finish saving %d structures to %s, have cost %d seconds' % (len(data_list), self.data_file, time.time() - begin))\n",
    "        \n",
    "\n",
    "    def process_worker(self, folder, **kwargs):\n",
    "        stru_id = os.path.split(folder)[-1] #用于将路径分割成头部和尾部两个部分。头部是路径中最后一个斜杠之前的部分（即父目录路径），而尾部是最后一个斜杠之后的部分（通常是文件名或最后一个目录名）。这里得到了每个结构的id：0-575\n",
    "        # print(\"process dir:\", folder)\n",
    "        structure = Structure(np.loadtxt(os.path.join(folder, 'lat.dat')), #加载第i个结构的晶格向量、原子序数和笛卡尔坐标\n",
    "                              np.loadtxt(os.path.join(folder, 'element.dat')),\n",
    "                              np.loadtxt(os.path.join(folder, 'site_positions.dat')),\n",
    "                              coords_are_cartesian=True,\n",
    "                              to_unit_cell=False) #用pymatgen创建结构\n",
    "\n",
    "        cart_coords = torch.tensor(structure.cart_coords, dtype=self.default_dtype_torch) #晶体的原子笛卡尔坐标，当前的默认浮点torch.dtype，36*3\n",
    "        frac_coords = torch.tensor(structure.frac_coords, dtype=self.default_dtype_torch)\n",
    "        numbers = torch.tensor(structure.atomic_numbers) #List of atomic numbers.tensor([36*83])，36个Bi原子的原子序数列表\n",
    "        # print(\"numbers = \", numbers)\n",
    "        # print(cart_coords)\n",
    "        structure.lattice.matrix.setflags(write=True) #描述structure.lattice.matrix是否可以写入。\n",
    "        lattice = torch.tensor(structure.lattice.matrix, dtype=self.default_dtype_torch) #读取structure.lattice.matrix，并转为default_dtype_torch类型\n",
    "        # print(lattice)\n",
    "        if self.target == 'E_ij':\n",
    "            huge_structure = True\n",
    "        else:\n",
    "            huge_structure = False # r=self.radius==-1。\n",
    "        return get_graph(cart_coords, frac_coords, numbers, stru_id, radius=self.radius, material_dimension=self.material_dimension,\n",
    "                         numerical_tol=1e-8, lattice=lattice, default_dtype_torch=self.default_dtype_torch,\n",
    "                         tb_folder=folder, data_format=self.data_format, num_l=self.num_l, shortest_path_length=self.shortest_path_length,\n",
    "                         if_lcmp_graph=self.if_lcmp_graph, separate_onsite=self.separate_onsite, target=self.target, \n",
    "                         huge_structure=huge_structure, if_new_sp=self.new_sp, **kwargs) #获得图类型的数据集return data\n",
    "        \n",
    "\n",
    "    def element_statistics(self, data_list):\n",
    "        #data_list[0].x是该晶体结构中所有原子的原子序数列表。torch.unique是从张量中提取不重复的值，即去重后排序。index_to_Z是去重排序后的原子序数列表；inverse_indices是原子序数列表中每个元素在去重排序后列表中的位置索引。\n",
    "        index_to_Z, inverse_indices = torch.unique(data_list[0].x, sorted=True, return_inverse=True)\n",
    "        #data_list[0].x是[36个83]的张量，index_to_Z将会是一个包含单一元素[83]的张量，因为所有元素都相同，去重后只剩下一个元素。\n",
    "        #inverse_indices将会是一个长度为36的张量，每个元素都是0，表示原始列表中的每个元素都对应去重后列表的第一个（也是唯一一个）元素。\n",
    "        Z_to_index = torch.full((100,), -1, dtype=torch.int64)#创建了一个形状为 (100,) 的张量，所有元素的值都是 -1，数据类型为 64 位整型。\n",
    "        Z_to_index[index_to_Z] = torch.arange(len(index_to_Z)) #torch.arange生成一个包含不同原子数的张量，包含从 0 开始逐步增加到len(index_to_Z)-1的整数序列\n",
    "        #Z_to_index[6] = 0， 如果有不同原子，则Z_to_index[26] = 1，以此类推\n",
    "        for data in data_list:#遍历所有晶体结构，把每个晶体结构的data.x都转为[36个0]\n",
    "            data.x = Z_to_index[data.x] #data.x是[36个83]的张量，Z_to_index[data.x]返回的是data.x作为每个索引下的值，36个0。如果存在不同原子，则把原子序数列表data.x(numbers)转为从0开始的不同元素的标识。\n",
    "\n",
    "        return index_to_Z, Z_to_index #[83], [第83个位置为0，其余为-1的(100,)张量]，如果存在不同原子，则不同原子的原子序数位置的值为1，2...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "629d442f-47ee-44e7-87f5-7c251f4d0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"basic\": {\n",
    "        \"raw_dir\": \"/fs2/home/ndsim10/DeepQT/0_generate_dataset/expand_dataset/raw/\",\n",
    "        \"processed_data_dir\": \"/fs2/home/ndsim10/DeepQT/0_generate_dataset/expand_dataset/processed/\",\n",
    "        \"graph_dir\": \"/fs2/home/ndsim10/DeepQT/0_generate_dataset/expand_dataset/graph/\",\n",
    "        \"target\": \"hamiltonian\",\n",
    "        \"interface\": \"siesta\",\n",
    "        \"data_format\": \"h5\",\n",
    "        \"input_file\": \"input.fdf\",\n",
    "        \"multiprocessing\": 0,\n",
    "        \"local_coordinate\": True,\n",
    "        \"material_dimension\": 2,\n",
    "    },\n",
    "    \"interpreter\": {\n",
    "        \"python_interpreter\": \"~/miniconda3/envs/deeph-cpu/bin/python\"\n",
    "    },\n",
    "    \"graph\": {\n",
    "        \"radius\": 7.0, #graphene 7.0 Å, MoS2 8.0 Å, and silicon  9.0 Å\n",
    "        \"num_l\": 4,\n",
    "        \"if_lcmp_graph\": True,\n",
    "        \"separate_onsite\": False,\n",
    "        \"new_sp\": False,\n",
    "        \"shortest_path_length\": 5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "856f6e66-06bd-4ace-8a54-f2cd6c3d8b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data file: HGraph-h5-4l.pkl\n",
      "Process new data file......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use multiprocessing (nodes = num_processors x num_threads = 1 x 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                          | 2/600 [00:06<34:55,  3.50s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_dtype_torch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#获取当前的默认浮点 torch.dtype。\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 70\u001b[0m, in \u001b[0;36mHData.__init__\u001b[0;34m(self, config, default_dtype_torch, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m begin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deeph-cpu/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeph-cpu/lib/python3.9/site-packages/torch_geometric/data/dataset.py:115\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeph-cpu/lib/python3.9/site-packages/torch_geometric/data/dataset.py:262\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    261\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    265\u001b[0m fs\u001b[38;5;241m.\u001b[39mtorch_save(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "Cell \u001b[0;32mIn[68], line 134\u001b[0m, in \u001b[0;36mHData.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse multiprocessing (nodes = num_processors x num_threads = 1 x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mget_num_threads()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#获得用于并行化CPU操作的OpenMP线程数，然后使用单进程运行。\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_worker(folder) \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(folder_list, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;66;03m#tqdm.tqdm()实现进度条，输入一个可迭代对象folder_list。输入每一个晶体结构文件夹到process_worker函数中。得到每一个晶体结构的图类型的数据，即一个晶体结构对应一个图data。\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     pool_dict \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing} \u001b[38;5;66;03m#如果self.multiprocessing>=0，pool_dict被赋值为包含一个键值对'nodes': self.multiprocessing 的字典。\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 134\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse multiprocessing (nodes = num_processors x num_threads = 1 x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mget_num_threads()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#获得用于并行化CPU操作的OpenMP线程数，然后使用单进程运行。\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(folder_list, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;66;03m#tqdm.tqdm()实现进度条，输入一个可迭代对象folder_list。输入每一个晶体结构文件夹到process_worker函数中。得到每一个晶体结构的图类型的数据，即一个晶体结构对应一个图data。\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     pool_dict \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing} \u001b[38;5;66;03m#如果self.multiprocessing>=0，pool_dict被赋值为包含一个键值对'nodes': self.multiprocessing 的字典。\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 183\u001b[0m, in \u001b[0;36mHData.process_worker\u001b[0;34m(self, folder, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     huge_structure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# r=self.radius==-1。\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcart_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrac_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstru_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaterial_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterial_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumerical_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_dtype_torch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_dtype_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtb_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshortest_path_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshortest_path_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mif_lcmp_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_lcmp_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparate_onsite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_onsite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mhuge_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuge_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_new_sp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DeepQT/1_preprocess/graph.py:376\u001b[0m, in \u001b[0;36mget_graph\u001b[0;34m(cart_coords, frac_coords, numbers, stru_id, radius, material_dimension, numerical_tol, lattice, default_dtype_torch, tb_folder, data_format, num_l, shortest_path_length, if_lcmp_graph, separate_onsite, target, huge_structure, only_get_R_list, if_new_sp, if_require_grad, fid_rc, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m         angular_expansion \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_l): \u001b[38;5;66;03m#num_l=0-4\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m             angular_expansion\u001b[38;5;241m.\u001b[39mappend(\u001b[43msph_harm_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_vec_sp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_vec_sp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#sph_harm_func.get返回tensor of shape [*theta.shape, 2*l+1]，涉及球谐函数，这里没搞懂。这个方法返回的结果是针对每个角度值(θ, φ)对应的球谐基函数展开，随后这些结果被追加到angular_expansion数组中。\u001b[39;00m\n\u001b[1;32m    377\u001b[0m         angular_expansion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(angular_expansion, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(edge_fea\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], edge_fea\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#torch.Size([2664, 2664, 16]，计算每个边在不同局域坐标系下的角度(θ, φ)的球谐基函数展开，这对于许多模拟和计算任务来说是非常有用的。这样的角度展开允许你在后续的计算中方便地使用球面函数的性质，比如在分子动力学模拟、光照计算或声场模拟中。\u001b[39;00m\n\u001b[1;32m    378\u001b[0m subgraph_atom_idx_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/DeepQT/1_preprocess/spherical_harmonics_basis.py:229\u001b[0m, in \u001b[0;36mSphericalHarmonics.get\u001b[0;34m(self, l, theta, phi, refresh)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# (l=0,m=0),(l=1,m=-1~1),(l=2,m=-2~2),(l=3,m=-3~3),(l=4,m=-4~4)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39ml, l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 229\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#返回张量形状\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(results, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/DeepQT/1_preprocess/spherical_harmonics_basis.py:200\u001b[0m, in \u001b[0;36mSphericalHarmonics.get_element\u001b[0;34m(self, l, m, theta, phi)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m l, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute value of order m must be <= degree l\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m N \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39ml\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi))\n\u001b[0;32m--> 200\u001b[0m leg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlpmv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m N\u001b[38;5;241m*\u001b[39mleg\n",
      "File \u001b[0;32m~/DeepQT/1_preprocess/spherical_harmonics_basis.py:172\u001b[0m, in \u001b[0;36mSphericalHarmonics.lpmv\u001b[0;34m(self, l, m, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlpmv(l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, m, x)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Compute P_{l}^m from recursion in P_{l-1}^m and P_{l-2}^m\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Inplace speedup\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mm_abs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlpmv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_abs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m-\u001b[39m m_abs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    174\u001b[0m     y \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m ((l\u001b[38;5;241m+\u001b[39mm_abs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m(l\u001b[38;5;241m-\u001b[39mm_abs)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleg[(l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, m_abs)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = HData(\n",
    "    config,\n",
    "    default_dtype_torch=torch.get_default_dtype() #获取当前的默认浮点 torch.dtype。\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92a988-a509-4aee-8ad7-c1c795abe9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeph-cpu)",
   "language": "python",
   "name": "deeph-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
